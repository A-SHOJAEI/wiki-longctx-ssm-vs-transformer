{
  "repo_name": "wiki-longctx-ssm-vs-transformer",
  "title": "Long-Context Wikipedia Language Modeling: Mamba (SSM) vs FlashAttention Transformers Under Equal Compute",
  "one_liner": "Download the latest English Wikipedia dump, build a reproducible long-context LM training corpus, then compare a decoder-only Transformer and a Mamba-style SSM language model on quality-per-FLOP, long-range metrics, and throughput.",
  "research_question": "When trained on the same number of tokens from Wikipedia under the same wall-clock and VRAM constraints, do modern state-space sequence models (Mamba/SSM) outperform FlashAttention Transformers in (1) validation perplexity and (2) long-range dependency retention, and what data-pipeline choices (packing, document boundaries, deduplication) drive the gains?",
  "dataset": {
    "name": "English Wikipedia XML dump (enwiki-latest pages-articles multistream)",
    "urls": [
      "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2",
      "https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream-index.txt.bz2"
    ],
    "license": "CC BY-SA 4.0 (with attribution requirements); historical content may also be under GFDL as noted by Wikimedia",
    "approx_size_gb": 23.0,
    "ingestion_notes": "Automated pipeline: download -> verify SHA256 (published alongside dumps) -> stream-decompress -> extract wikitext to plain text (WikiExtractor-compatible) -> filter (min chars, remove boilerplate lists/tables), language-id sanity check -> deterministic document-level split (hash(doc_id) into train/val/test) -> train a SentencePiece BPE tokenizer on a fixed random sample -> tokenize and write shard files (e.g., WebDataset/JSONL+zstd) with document boundary markers -> build two dataloaders: (A) packed sequences across docs, (B) doc-boundary-respecting packing with optional reset masks."
  },
  "method": {
    "model": "Mamba/SSM decoder-only LM (~130M params) in PyTorch with bf16/fp16, fused kernels where available, trained with long context (4096 tokens), sequence packing, gradient checkpointing, and DDP across 2x3090; objective is next-token prediction with document boundary tokens and optional reset masks.",
    "baseline": "GPT-style decoder-only Transformer LM (~130M params) in PyTorch trained on the exact same token stream and optimizer schedule; variants: (1) standard attention, (2) FlashAttention2-enabled attention (this is the primary Transformer baseline).",
    "ablations": [
      "No packing (fixed-length chunks, drop remainder) vs packed sequences (maximize token utilization).",
      "No document-boundary markers vs boundary markers + reset masks (prevents cross-document leakage).",
      "No near-duplicate filtering vs MinHash/SimHash-based document deduplication (kept compute constant by replacing removed docs with new ones).",
      "Context length 1024 vs 2048 vs 4096 (hold total trained tokens constant; measure long-range metric degradation curves)."
    ],
    "metrics": [
      "Validation perplexity (and NLL) on held-out Wikipedia documents (fixed tokenizer, fixed split).",
      "Long-range retention: contrastive suffix-identification task created from held-out docs (accuracy vs distance buckets, e.g., 128/512/1024/2048/4096 tokens).",
      "Throughput: tokens/sec and achieved GPU utilization; peak VRAM; training stability (loss spikes, grad norm).",
      "Quality-per-compute: perplexity vs estimated FLOPs and vs wall-clock (Pareto comparison)."
    ]
  },
  "compute": {
    "gpus": "2x NVIDIA GeForce RTX 3090 (24GB each), DDP",
    "expected_hours": 24.0
  },
  "risks": [
    "Disk usage: extracted text + token shards can exceed 100GB depending on choices; needs automatic cleanup and configurable shard sizes.",
    "Wikipedia dump contents change over time; using enwiki-latest reduces long-term reproducibility unless the pipeline pins dump date/URL and records checksums.",
    "Preprocessing can be CPU-heavy; malformed pages and extractor edge cases must be handled to avoid silent data corruption.",
    "Fairness of comparison: matching parameter counts is insufficient; must also match token budget, optimizer, and effective batch size to make conclusions defensible.",
    "Kernel/library variability (FlashAttention, mamba-ssm) can affect throughput and stability; must log versions and provide a fallback path."
  ],
  "execution_steps": [
    "Create env and install deps: `python -m venv .venv && source .venv/bin/activate && pip install -U pip && pip install -r requirements.txt` (PyTorch CUDA, sentencepiece, zstandard, tqdm, numpy, mamba-ssm, flash-attn optional, tensorboard).",
    "Download + verify dump: `python scripts/download_wiki.py --out data/raw --dump enwiki-latest` (records URLs, sizes, SHA256).",
    "Extract + clean: `python scripts/extract_wiki.py --in data/raw/...xml.bz2 --out data/clean --workers 24`.",
    "Build tokenizer: `python scripts/train_tokenizer.py --in data/clean --out data/tokenizer --vocab_size 32000 --sample_bytes 2000000000 --seed 1234`.",
    "Tokenize + shard: `python scripts/tokenize_shard.py --in data/clean --tokenizer data/tokenizer/sp.model --out data/shards --shard_size_tokens 50000000 --split_hash`.",
    "Train baseline Transformer: `torchrun --nproc_per_node=2 train.py --config configs/transformer_130m.yaml --data data/shards --logdir runs/transformer`.",
    "Train proposed Mamba/SSM: `torchrun --nproc_per_node=2 train.py --config configs/mamba_130m.yaml --data data/shards --logdir runs/mamba`.",
    "Run ablations by swapping configs (packing/doc-boundary/dedup/context): `torchrun --nproc_per_node=2 train.py --config ...`.",
    "Evaluate: `python eval/perplexity.py --ckpt runs/... --data data/shards/val` and `python eval/long_range_suffix_id.py --ckpt runs/... --data data/shards/test`.",
    "Produce a research report (tables + plots + reproducibility manifest): `python tools/make_report.py --runs runs --out report/`."
  ],
  "generated_at_utc": "2026-02-10 04:38:03 UTC",
  "hardware": {
    "cpu_cores": 24,
    "cpu_threads": 48,
    "ram_gb": 251.59,
    "gpu_count": 2,
    "gpu_names": [
      "NVIDIA GeForce RTX 3090",
      "NVIDIA GeForce RTX 3090"
    ],
    "gpu_vram_gb": [
      24.0,
      24.0
    ]
  }
}