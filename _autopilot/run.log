
[2026-02-10 04:58:53 UTC] autofix: patched Makefile setup for venv bootstrap/PEP668

[2026-02-10 04:58:53 UTC] $ make setup
NOTE: GETPIP_SHA256 not set; skipping checksum verification for get-pip.py
Collecting pip
  Using cached pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)
Using cached pip-26.0.1-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
Successfully installed pip-26.0.1
Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (26.0.1)
Collecting setuptools
  Using cached setuptools-82.0.0-py3-none-any.whl.metadata (6.6 kB)
Collecting wheel
  Using cached wheel-0.46.3-py3-none-any.whl.metadata (2.4 kB)
Collecting packaging>=24.0 (from wheel)
  Using cached packaging-26.0-py3-none-any.whl.metadata (3.3 kB)
Using cached setuptools-82.0.0-py3-none-any.whl (1.0 MB)
Using cached wheel-0.46.3-py3-none-any.whl (30 kB)
Using cached packaging-26.0-py3-none-any.whl (74 kB)
Installing collected packages: setuptools, packaging, wheel

Successfully installed packaging-26.0 setuptools-82.0.0 wheel-0.46.3
Collecting numpy==1.26.4 (from -r requirements.txt (line 2))
  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)
Collecting requests==2.32.3 (from -r requirements.txt (line 3))
  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)
Collecting tqdm==4.66.4 (from -r requirements.txt (line 4))
  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)
Collecting PyYAML==6.0.1 (from -r requirements.txt (line 5))
  Using cached PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)
Collecting sentencepiece==0.2.0 (from -r requirements.txt (line 6))
  Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)
Collecting zstandard==0.22.0 (from -r requirements.txt (line 7))
  Downloading zstandard-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)
Collecting regex==2024.5.15 (from -r requirements.txt (line 8))
  Downloading regex-2024.5.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)
Collecting torch==2.4.1 (from -r requirements.txt (line 11))
  Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)
Collecting mwxml==0.3.3 (from -r requirements.txt (line 14))
  Downloading mwxml-0.3.3-py2.py3-none-any.whl.metadata (2.2 kB)
Collecting mwparserfromhell==0.6.6 (from -r requirements.txt (line 15))
  Using cached mwparserfromhell-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)
Collecting charset-normalizer<4,>=2 (from requests==2.32.3->-r requirements.txt (line 3))
  Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)
Collecting idna<4,>=2.5 (from requests==2.32.3->-r requirements.txt (line 3))
  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)
Collecting urllib3<3,>=1.21.1 (from requests==2.32.3->-r requirements.txt (line 3))
  Using cached urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)
Collecting certifi>=2017.4.17 (from requests==2.32.3->-r requirements.txt (line 3))
  Using cached certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)
Collecting filelock (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)
Collecting typing-extensions>=4.8.0 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Collecting sympy (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)
Collecting jinja2 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting fsspec (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)
Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.4.1->-r requirements.txt (line 11)) (82.0.0)
Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)
Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)
Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)
Collecting triton==3.0.0 (from torch==2.4.1->-r requirements.txt (line 11))
  Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)
Collecting jsonschema>=2.5.1 (from mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached jsonschema-4.26.0-py3-none-any.whl.metadata (7.6 kB)
Collecting mwcli>=0.0.2 (from mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached mwcli-0.0.3-py2.py3-none-any.whl.metadata (1.2 kB)
Collecting mwtypes>=0.3.0 (from mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached mwtypes-0.4.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting para>=0.0.1 (from mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached para-0.0.8-py3-none-any.whl.metadata (2.0 kB)
Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.1->-r requirements.txt (line 11))
  Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting attrs>=22.2.0 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)
Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)
Collecting referencing>=0.28.4 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)
Collecting rpds-py>=0.25.0 (from jsonschema>=2.5.1->mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached rpds_py-0.30.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting docopt (from mwcli>=0.0.2->mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached docopt-0.6.2-py2.py3-none-any.whl
Collecting jsonable>=0.3.0 (from mwtypes>=0.3.0->mwxml==0.3.3->-r requirements.txt (line 14))
  Using cached jsonable-0.3.1-py2.py3-none-any.whl.metadata (1.6 kB)
Collecting MarkupSafe>=2.0 (from jinja2->torch==2.4.1->-r requirements.txt (line 11))
  Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)
Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.4.1->-r requirements.txt (line 11))
  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)
Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)
Using cached requests-2.32.3-py3-none-any.whl (64 kB)
Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)
Using cached PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)
Using cached sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)
Downloading zstandard-0.22.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 14.9 MB/s  0:00:00
Downloading regex-2024.5.15-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (788 kB)
   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 788.8/788.8 kB 16.5 MB/s  0:00:00
Using cached torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)
Downloading mwxml-0.3.3-py2.py3-none-any.whl (32 kB)
Using cached mwparserfromhell-0.6.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)
Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)
Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)
Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)
Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)
Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)
Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)
Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)
Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)
Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)
Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)
Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)
Using cached triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)
Using cached charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)
Using cached idna-3.11-py3-none-any.whl (71 kB)
Using cached urllib3-2.6.3-py3-none-any.whl (131 kB)
Using cached certifi-2026.1.4-py3-none-any.whl (152 kB)
Using cached jsonschema-4.26.0-py3-none-any.whl (90 kB)
Using cached attrs-25.4.0-py3-none-any.whl (67 kB)
Using cached jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)
Using cached mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)
Using cached mwtypes-0.4.0-py2.py3-none-any.whl (20 kB)
Using cached jsonable-0.3.1-py2.py3-none-any.whl (11 kB)
Using cached para-0.0.8-py3-none-any.whl (6.5 kB)
Using cached referencing-0.37.0-py3-none-any.whl (26 kB)
Using cached rpds_py-0.30.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (394 kB)
Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)
Using cached filelock-3.20.3-py3-none-any.whl (16 kB)
Using cached fsspec-2026.2.0-py3-none-any.whl (202 kB)
Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)
Using cached markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)
Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)
Using cached nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)
Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)
Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)
Installing collected packages: sentencepiece, para, mpmath, jsonable, docopt, zstandard, urllib3, typing-extensions, tqdm, sympy, rpds-py, regex, PyYAML, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mwtypes, mwparserfromhell, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, attrs, triton, requests, referencing, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, jsonschema-specifications, torch, jsonschema, mwcli, mwxml

Successfully installed MarkupSafe-3.0.3 PyYAML-6.0.1 attrs-25.4.0 certifi-2026.1.4 charset-normalizer-3.4.4 docopt-0.6.2 filelock-3.20.3 fsspec-2026.2.0 idna-3.11 jinja2-3.1.6 jsonable-0.3.1 jsonschema-4.26.0 jsonschema-specifications-2025.9.1 mpmath-1.3.0 mwcli-0.0.3 mwparserfromhell-0.6.6 mwtypes-0.4.0 mwxml-0.3.3 networkx-3.6.1 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 para-0.0.8 referencing-0.37.0 regex-2024.5.15 requests-2.32.3 rpds-py-0.30.0 sentencepiece-0.2.0 sympy-1.14.0 torch-2.4.1 tqdm-4.66.4 triton-3.0.0 typing-extensions-4.15.0 urllib3-2.6.3 zstandard-0.22.0

[2026-02-10 04:59:35 UTC] $ make all
sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=data/smoke/tokenizer/sp_input.txt --model_prefix=data/smoke/tokenizer/sp --model_type=bpe --vocab_size=8000 --character_coverage=1.0 --input_sentence_size=1000000 --shuffle_input_sentence=true --random_seed=1234 --unk_id=0 --bos_id=1 --eos_id=2 --pad_id=3 --user_defined_symbols=<DOC> --hard_vocab_limit=false
Traceback (most recent call last):
  File "/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/scripts/make_smoke_data.py", line 104, in <module>
    main()
  File "/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/scripts/make_smoke_data.py", line 77, in main
    train_sentencepiece(
  File "/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/scripts/train_tokenizer.py", line 60, in train_sentencepiece
    spm.SentencePieceTrainer.Train(" ".join(args))
  File "/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py", line 1047, in Train
    SentencePieceTrainer._Train(arg=arg, **kwargs)
  File "/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py", line 1003, in _Train
    return SentencePieceTrainer._TrainFromString(arg)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/.venv/lib/python3.12/site-packages/sentencepiece/__init__.py", line 981, in _TrainFromString
    return _sentencepiece.SentencePieceTrainer__TrainFromString(arg)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: Not found: unknown field name "random_seed" in TrainerSpec.
make: *** [Makefile:23: data] Error 1

[2026-02-10 05:02:04 UTC] $ make all
/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/src/wiki_lm/train_loop.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(path, map_location="cpu")
/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/src/wiki_lm/train_loop.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == torch.float16 and device.type == "cuda"))
/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/src/wiki_lm/train_loop.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(path, map_location="cpu")
/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/src/wiki_lm/train_loop.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == torch.float16 and device.type == "cuda"))
/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/src/wiki_lm/train_loop.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ckpt = torch.load(path, map_location="cpu")
/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/src/wiki_lm/train_loop.py:245: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == torch.float16 and device.type == "cuda"))
/home/alireza/research-autopilot/generated_projects/wiki-longctx-ssm-vs-transformer/evaluate.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  return torch.load(p, map_location="cpu")
